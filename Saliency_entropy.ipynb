{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFJyFk2qL2L1"
      },
      "source": [
        "- multiple models that get good accuracy\n",
        "- but we need the model with the least weights\n",
        "- get linearly dependent data by transforming. fetch y by some model\n",
        "- train some model directly and look at the data gradients (average them)\n",
        "- train model with saliency and look at the data gradients (average them)\n",
        "\n",
        "See there is a difference in grads but not the converged loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4GkA_PECPNPj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from itertools import chain, combinations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from torch.autograd import grad, Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import sys\n",
        "from common import SubprocVecEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM5l_S3SPQ4k",
        "outputId": "7fdb7da9-763e-4955-b770-f0464a199f70"
      },
      "outputs": [],
      "source": [
        "num_envs = 16\n",
        "env_name = \"CartPole-v1\"\n",
        "\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        env = gym.make(env_name, new_step_api=False)\n",
        "        return env\n",
        "\n",
        "    return _thunk\n",
        "\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "\n",
        "env = gym.make(env_name, new_step_api=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z-sKOYvkPUpH"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        print(\"Network configuration:\")\n",
        "        print(\"input_dim:\", num_inputs)\n",
        "        print(\"hidden_dim:\", hidden_size)\n",
        "        print(\"output_dim:\", num_outputs)\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size), nn.ReLU(), nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_outputs),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        probs = self.actor(x)\n",
        "        dist = Categorical(probs)\n",
        "        return dist, value, probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Hparams:\n",
        "    num_inputs = envs.observation_space.shape[0]\n",
        "    num_outputs = envs.action_space.n\n",
        "    hidden_size = 64\n",
        "    lr = 1e-3\n",
        "    num_steps = 10\n",
        "    max_frames = 20000\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ClEAZuh9PXDj"
      },
      "outputs": [],
      "source": [
        "def test_env(model, transform=None):\n",
        "    if transform is None:\n",
        "        state = env.reset()\n",
        "    else:\n",
        "        state = transform(env.reset())\n",
        "\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(Hparams.device)\n",
        "        dist, _, _ = model(state)\n",
        "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
        "        if transform is None:\n",
        "            state = next_state\n",
        "        else:\n",
        "            state = transform(next_state)\n",
        "        total_reward += reward\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BNs0X8XNPYlU"
      },
      "outputs": [],
      "source": [
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "    R = next_value\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        R = rewards[step] + gamma * R * masks[step]\n",
        "        returns.insert(0, R)\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LossObject:\n",
        "    def __init__(self, experiment):\n",
        "        self.reset()\n",
        "        self.frame_idx = 0\n",
        "        self.writer = SummaryWriter(f'logs/{experiment}/')\n",
        "\n",
        "    def reset(self):\n",
        "        self.log_probs = []\n",
        "        self.values = []\n",
        "        self.rewards = []\n",
        "        self.masks = []\n",
        "        self.grad_reg = []\n",
        "        self.entropy = 0\n",
        "            \n",
        "    def update(self, log_prob, value, reward, done, grad_reg_entropy, entropy):\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.values.append(value)\n",
        "        self.rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(Hparams.device))\n",
        "        self.masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(Hparams.device))\n",
        "        self.grad_reg.append(grad_reg_entropy)\n",
        "        self.entropy += entropy\n",
        "        self.frame_idx += 1\n",
        "\n",
        "    def compute_loss(self, next_value, gamma=0.99):\n",
        "        R = next_value\n",
        "        returns = []\n",
        "        for step in reversed(range(len(self.rewards))):\n",
        "            R = self.rewards[step] + gamma * R * self.masks[step]\n",
        "            returns.insert(0, R)\n",
        "        \n",
        "        grad_reg = torch.cat(self.grad_reg)\n",
        "        log_probs = torch.cat(self.log_probs)\n",
        "        returns = torch.cat(returns).detach()\n",
        "        values = torch.cat(self.values)\n",
        "\n",
        "        advantage = returns - values\n",
        "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "        critic_loss = advantage.pow(2).mean()\n",
        "        grad_reg = grad_reg.mean()\n",
        "        loss_without_critic = actor_loss - 0.001 * self.entropy\n",
        "        loss = loss_without_critic + 0.5 * critic_loss + grad_reg\n",
        "\n",
        "        # log actor loss, entropy and grad_reg\n",
        "        self.writer.add_scalar(\"actor_loss\", actor_loss.item(), self.frame_idx)\n",
        "        self.writer.add_scalar(\"entropy\", self.entropy.item(), self.frame_idx)\n",
        "        self.writer.add_scalar(\"grad_reg\", grad_reg.item(), self.frame_idx)\n",
        "        self.writer.add_scalar(\"loss_without_critic\", loss_without_critic.item(), self.frame_idx)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sBrU-joYPhUS"
      },
      "outputs": [],
      "source": [
        "def model_env_forward(model, state, envs):\n",
        "    state = torch.FloatTensor(state).to(Hparams.device)\n",
        "    state.requires_grad = True\n",
        "    dist, value, probs = model(state)\n",
        "\n",
        "    action = dist.sample()\n",
        "    entropy = dist.entropy().mean()\n",
        "    next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "\n",
        "    log_prob = dist.log_prob(action)\n",
        "\n",
        "    saliency = grad(probs[:,0].sum(), state, retain_graph=True, create_graph=True)[0]\n",
        "    saliency = torch.softmax(saliency**2, axis=1)\n",
        "    grad_reg_entropy = - (saliency * torch.log(saliency)).sum(axis=1).mean(axis=0, keepdim=True)\n",
        "\n",
        "    return next_state, log_prob, value, reward, done, grad_reg_entropy, entropy\n",
        "\n",
        "def train_model(experiment):\n",
        "    transform = None\n",
        "\n",
        "    # init model and optimizer\n",
        "    model = ActorCritic(\n",
        "        num_inputs=Hparams.num_inputs,\n",
        "        num_outputs=Hparams.num_outputs,\n",
        "        hidden_size=Hparams.hidden_size,\n",
        "    ).to(Hparams.device)\n",
        "\n",
        "    optimizer = optim.Adam(lr=Hparams.lr, params=model.parameters())\n",
        "\n",
        "    # init env\n",
        "    max_avg_reward = 0\n",
        "    state = envs.reset()\n",
        "    loss_obj = LossObject(experiment)\n",
        "    \n",
        "    while loss_obj.frame_idx < Hparams.max_frames:\n",
        "        loss_obj.reset()\n",
        "\n",
        "        for _ in range(Hparams.num_steps):\n",
        "            state, *update_args = model_env_forward(model, state, envs)\n",
        "            loss_obj.update(*update_args)\n",
        "\n",
        "            if loss_obj.frame_idx % 500 == 0:\n",
        "                # average reward over 500 episodes\n",
        "                avg_reward = np.mean([test_env(model) for _ in range(500)])\n",
        "                max_avg_reward = max(max_avg_reward, avg_reward)\n",
        "                print(f\"Frame={loss_obj.frame_idx} => avg_reward={avg_reward:.4f}\")\n",
        "                loss_obj.writer.add_scalar('avg_reward', avg_reward, loss_obj.frame_idx)\n",
        "\n",
        "        state = torch.FloatTensor(state).to(Hparams.device)\n",
        "        next_value= model(state)[1]\n",
        "\n",
        "        loss = loss_obj.compute_loss(next_value)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_reward = np.mean([test_env(model) for _ in range(500)])\n",
        "    max_avg_reward = max(max_avg_reward, avg_reward)\n",
        "    return max_avg_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNesFHiGQhdM"
      },
      "source": [
        "episode wise loop and avg across training loop\n",
        "\n",
        "where regularization\n",
        "how does training look like\n",
        "concretely formalize idea\n",
        "\n",
        "saliency?\n",
        "algorithm? ac or dqn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj24ql3paYsI"
      },
      "source": [
        "log rewards\n",
        "loss\n",
        "entropy\n",
        "\n",
        "best model we track\n",
        "reward\n",
        "loss_without_entropy\n",
        "entropy\n",
        "grads of states in test rollouts (average across states)\n",
        "perturb useless input then train model using both methods (with and without entropy) then see model does better with entropy\n",
        "\n",
        "understand scale of loss and entropy\n",
        "use lightning and tensorboard to log them\n",
        "start with high weightage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfeWOU3xQh_P",
        "outputId": "0dc15196-4d9c-4566-df25-308124c5ccab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network configuration:\n",
            "input_dim: 4\n",
            "hidden_dim: 64\n",
            "output_dim: 2\n",
            "Frame=500 => avg_reward=21.3480\n",
            "Frame=1000 => avg_reward=21.8260\n",
            "Frame=1500 => avg_reward=21.2680\n",
            "Frame=2000 => avg_reward=20.3400\n",
            "Frame=2500 => avg_reward=21.7620\n",
            "Frame=3000 => avg_reward=22.0940\n",
            "Frame=3500 => avg_reward=23.3360\n",
            "Frame=4000 => avg_reward=22.6200\n",
            "Frame=4500 => avg_reward=22.0280\n",
            "Frame=5000 => avg_reward=22.4880\n",
            "Frame=5500 => avg_reward=23.7820\n",
            "Frame=6000 => avg_reward=24.8560\n",
            "Frame=6500 => avg_reward=27.8140\n",
            "Frame=7000 => avg_reward=31.3820\n",
            "Frame=7500 => avg_reward=33.9120\n",
            "Frame=8000 => avg_reward=37.6680\n",
            "Frame=8500 => avg_reward=45.6360\n",
            "Frame=9000 => avg_reward=45.0800\n",
            "Frame=9500 => avg_reward=52.5980\n",
            "Frame=10000 => avg_reward=57.0740\n",
            "Frame=10500 => avg_reward=63.8300\n",
            "Frame=11000 => avg_reward=77.3360\n",
            "Frame=11500 => avg_reward=120.1260\n",
            "Frame=12000 => avg_reward=124.4800\n",
            "Frame=12500 => avg_reward=143.3200\n",
            "Frame=13000 => avg_reward=79.7360\n",
            "Frame=13500 => avg_reward=96.1280\n",
            "Frame=14000 => avg_reward=105.2140\n",
            "Frame=14500 => avg_reward=121.1440\n",
            "Frame=15000 => avg_reward=134.4180\n",
            "Frame=15500 => avg_reward=179.4340\n",
            "Frame=16000 => avg_reward=156.9980\n",
            "Frame=16500 => avg_reward=136.2980\n",
            "Frame=17000 => avg_reward=132.1080\n",
            "Frame=17500 => avg_reward=128.2760\n",
            "Frame=18000 => avg_reward=103.4760\n",
            "Frame=18500 => avg_reward=73.8540\n",
            "Frame=19000 => avg_reward=56.5340\n",
            "Frame=19500 => avg_reward=29.9880\n",
            "Frame=20000 => avg_reward=22.1940\n"
          ]
        }
      ],
      "source": [
        "avg_reward = train_model('cartpole_v1_grad_reg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Saliency entropy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('research')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0b7b687ce3852753d26a6797c8bcc6f0dbaad735439f8fde9f3058628612a70"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
